
## Description
Gaming Workout Hub is an exercise website that allows gamers to exercise while gaming, prioritizing customizability and tracking features based on the user. This website will make exercising at home fun and easy by letting gamers play games and get alarms of when to exercise and or check off a list of exercises they can customize. The gamer will be able to customize exactly what they want for their workout, allowing for a flexible schedule that caters to the gamers needs and wants.


## Verification
Verification aims to ensure that you correctly developed the product. 

For this deliverable, show an example of a unit test that uses mock objects to isolate the class from the rest of the system. 

Test framework you used to develop your tests (e.g., JUnit, unittest, pytest, etc.)
Link to your GitHub folder where your automated unit tests are located.
An example of a test case that makes use of mock objects. Include in your answer a GitHub link to the class being tested and to the test.
A print screen showing the result of the unit tests execution. 
Grading criteria (5 points): adequate choice of a test framework, coverage of the tests, quality of the tests, adequate use of Mock objects, and a print screen showing successful test execution.

## Acceptance Test
For the acceptance tests, we used Selenium to test the user interface, specifically with Google Chrome and the Chrome webdriver.  
In our instance, we decided to use Python but had some difficulties using pip and installing Selenium on a Mac, so we had to create our own virtual environment to install Selenium and run the Python scripts to work with the Chrome web driver.  

We specifically chose to test the workouts and body test pages because they utilize user interaction the most.  

For the workouts page, we wanted to ensure that both implementations of the workouts would be generated correctly. As shown in the screenshots below, we created some simple functions to quickly go through the workout selection and confirm the choice, while also ensuring that the correct pages are being shown.. Selenium waits for the correct element of the page to show up, and then checks the header to ensure the correct details are being shown. After it iterates through the selection and confirmation questions, it first tests the interval-based workouts. For simplicity, it puts 3 seconds into the timer and then checks that a workout is generated by looking for "Your workout is:", ensuring that a workout has been generated after the interval. Similarly, it checks the round-based workouts by reloading the page and goes through the same selection and confirmation questions, inputs a 10, and then checks that a workout is generated.  

[Link to the Code](https://github.com/amb2893/CS386Project/blob/testHTML/PyTests/Selenium/workoutPlan.py)

The body test is much simpler. Selenium first gathers all of the buttons that are shown to the user, and then selects at least one option for each question, and then clicks submit. After this, it checks that there are exactly 3 daily workout recommendations based on the user's answers. Immediately after, Selenium refreshes the page to check for the correct behavior if there is not a selection for each question. It selects an option for the first few questions and then hits submit. Selenium then checks that an alert has popped up and the recommendations have not been generated, ensuring the correct logic of the website.

[Link to the Code](https://github.com/amb2893/CS386Project/blob/testHTML/PyTests/Selenium/bodyTest.py)

There is very simple output from Selenium after each successful test, but the assertions in the code would prevent these messages from being shown if they did not pass the tests.  
Messages after successful tests:  
![Successful Tests](https://github.com/amb2893/CS386Project/blob/testHTML/Deliverables/D-7-Pictures/successful-tests.png)

## Validation
At the beginning of the semester, you talked to the clients/potential users to understand their needs. Now it is time to check if you are on the right track by conducting some user evaluation on the actual system. Include in this deliverable the following information:

Script: The script should have the tasks that you gave to the user, what data you collected, and the questions you asked. In particular, do not forget to add questions about the users’ general impressions. You can ask open questions (e.g., How would you describe the homepage of our app? How do you compare our system to the competitor X?) or closed questions (On a scale of 1 to 10, how would you rate the layout of our application? On the same scale, how likely would you use the system in its current state?). Take a look at the inception and requirements deliverables to help create the script. Design a script to check if you are achieving your initial goals and if the features are implemented in a satisfactory way. 

Results: Conduct the user evaluation with at least 3 users. Report the data that you collected.
### Interview 1: Gabby Reinert (4/25/25)
### Interviewee: Anthony


### Data Collected 
**Signing up:**
- How was the sign up process? Pretty easy
- Were there any errors or bugs? no!
- What did you like about it? I liked that it let me go to register from login page and then brought me back to login
- Was anything confusing? No 


**Beginning a workout:**
- Was it easy to start a workout? Yes
- Were there any errors or bugs? no
- What did you like about it? Easy, fun
- Was anything confusing? No back button to go to the workout page again


**Complete it your works:**
- Did you enjoy the progression of the workouts given? yeah
- Were there any errors or bugs? no
- What did you like about it? Interesting way to do it
- Was anything confusing? no
- Were the workouts appealing? Good selection

**Navigation:**
- Was it easy the navigate through the site? yes
- Are the noticeable errors in the sites layout? Features on home page are not buttons
- What did you like about it? Easy to understand and use
- Was anything confusing? Alarm system is confusing
- Was the site color ways appealing? Easy on the eyes yes




### Interview 2: Gregory Thomas (4/26/25)
### Interviewee: Luke Shahan


### Data Collected 
**Signing up:**
- How was the sign up process? "Very easy"
- Were there any errors or bugs? "Not any I could find"
- What did you like about it? "I love the design of the website. It gives off the style of a gamer"
- Was anything confusing? "Nope"


**Beginning a workout:**
- Was it easy to start a workout? "Yes, I was a little confused at first, but then I understood how to start it"
- Were there any errors or bugs? "No"
- What did you like about it? "It's a habit to start on"
- Was anything confusing? "Just some of the pronunciation"


**Complete it your works:**
- Did you enjoy the progression of the workouts given? "Yes I find that a good tool to use"
- Were there any errors or bugs? "No"
- What did you like about it? "I like how it tracks how many workouts I've completed"
- Was anything confusing? "no"
- Were the workouts appealing? "Yes"

**Navigation:**
- Was it easy the navigate through the site? "Yes"
- Are the noticeable errors in the sites layout? "I thought the features would navigate you through the website"
- What did you like about it? "Everything is on the top so it is easy to scroll through"
- Was anything confusing? "The notfication section is iffy and doesn't make sense to me"
- Was the site's color ways appealing? "Yes I like purple"

### Interview 3: Chris Schanzer (4/25/25)
### Interviewee: Franz


### Data Collected
**Signing up:**
- I thought the sign up process was simple as any other sign up process.
- I didn’t see any errors. 
- The simplicity. 
- Not confusing.

**Beginning a workout:**
- Yes it was easy to start a workout.
- I didn’t notices any errors.
- I liked that I was any to pick and chose.
- Nothing confusing.


**Complete it your works:**
- The profession bar is helpful and nice. 
- No bugs. 
- I liked everything. Nothing confusing. 
- The workout were easy to do at home.

**Navigation:**
- I was easy to navigate through the website.
- I noticed there was an error which the link under the title on the page.
- The progression bar was neat.
- Other than that it was easy and aesthetically appealing.


Reflections: Reflect on what you observed. Some questions that you can explore: What features worked well? What can be changed? How is the learning curve of your system? Did the users perform the tasks as you expected? Did the users’ actions produce the results they expected? What did the users like the most? Is your value proposition accomplished? 

Grading criteria (17 points): adequate script, adequate report of the results, adequate reflection, language.
